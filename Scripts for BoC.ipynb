{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------- Import packages -------- #\n",
    "import numpy as np \n",
    "import numpy.ma as ma\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import graph_tool.all as gt\n",
    "import sys, argparse, csv, math\n",
    "from dateutil.parser import parse \n",
    "from unidecode import unidecode \n",
    "import datetime \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-------- Agent-based model of social hierarchy: main script -------- #\n",
    "\n",
    "#Set model parameters\n",
    "delta = 0.2\n",
    "alpha = 0.2\n",
    "eta = 0.5\n",
    "eps = 0.1\n",
    "\n",
    "#Set simulation parameters\n",
    "N = 1000\n",
    "ic = 'Egal'\n",
    "step=N     \n",
    "T=1000*N\n",
    "maxS = 2\n",
    "Sav = maxS/2.\n",
    "nr=10\n",
    "\n",
    "#Initiate output arrays\n",
    "S_av = np.zeros(N) \n",
    "m2 = np.zeros(T/step)\n",
    "m2_av = np.zeros(T/step)\n",
    "m3 = np.zeros(T/step)\n",
    "m3_av = np.zeros(T/step)\n",
    "m4 = np.zeros(T/step)\n",
    "m4_av = np.zeros(T/step)\n",
    "dirout = '/home/jhickey/outputdir/'\n",
    "\n",
    "#Loop over nr realizations of the simulation\n",
    "start_time=time.time()\n",
    "for run in range(0,nr):   \n",
    "\n",
    "    #Print progress to screen\n",
    "    if run%(nr/10)==0: print \"run =\", run, \"time elapsed =\", np.round(time.time()-start_time,0)\n",
    "\n",
    "    #Assign a status to each agent according to the initial condition\n",
    "    if ic == 'Egal':\n",
    "        S = np.ones(N)*(maxS/2.*N/N) #Egalitarian (delta-function) initial status\n",
    "    elif ic == 'Unif':\n",
    "        S = np.sort(np.random.random_sample(size=N)*maxS) #Random initial status, drawn from uniform distn\n",
    "        \n",
    "    #Loop over simulation time steps\n",
    "    for t in range(0,T+1):    \n",
    "\n",
    "        #Pick pair of agents randomly (np.random.choice() is too slow)\n",
    "        pair=np.array([-1,-1])\n",
    "        r = np.random.random_sample()\n",
    "        pair[0]=int(np.floor(r*N))\n",
    "        while pair[1] < 0:\n",
    "            idx=0\n",
    "            r = np.random.random_sample()\n",
    "            val = int(np.floor(r*N))\n",
    "            if  val != pair[0]: pair[1] = val\n",
    "\n",
    "        #Assign higher status of pair to Sidx1 and lower status to Sidx2\n",
    "        strengths = np.array([S[pair[0]],S[pair[1]]])        \n",
    "        Sidx1 =  pair[np.argsort(strengths)[1]]\n",
    "        S1 = S[Sidx1]\n",
    "        Sidx2 = pair[np.argsort(strengths)[0]]\n",
    "        S2 = S[Sidx2]   \n",
    "\n",
    "        #Criterion for determining if fight takes place\n",
    "        fight_occurs=False\n",
    "        if ((S1-S2) < eta*Sav) or (np.random.random_sample() < eps): fight_occurs=True\n",
    "        \n",
    "        if fight_occurs == True:  \n",
    "            \n",
    "            #Calculate p\n",
    "            p = 1./(1+(S2/S1)**alpha)                  \n",
    "            r = np.random.random_sample()\n",
    "\n",
    "            if r <= p: #Sidx1 (higher-status agent) wins fight\n",
    "                S[Sidx1] += S2*delta\n",
    "                S[Sidx2] -= S2*delta\n",
    "\n",
    "            elif r > p: #Sidx2 (lower-status agent) wins fight       \n",
    "                S[Sidx1] -= S1*delta\n",
    "                S[Sidx2] += S1*delta    \n",
    "\n",
    "        if t%step == 0: \n",
    "            i = t/step\n",
    "            m2[i] = stats.moment(S,moment=2)\n",
    "            m3[i] = stats.moment(S,moment=3)                         \n",
    "\n",
    "    #Build up quantities to be averaged over the nr realizations\n",
    "    m2_av += m2\n",
    "    m3_av += m3                    \n",
    "    S_av += np.sort(S)      \n",
    "\n",
    "#Calculate averages over all realizations\n",
    "m2_av = m2_av/float(nr)\n",
    "m3_av = m3_av/float(nr)                        \n",
    "S_av = S_av/float(nr)                 \n",
    "\n",
    "#Save averaged variables S_av, m2_av, m3_av\n",
    "fstring = '_N'+str(N)+'_alpha'+str(alpha)+'_delta'+str(delta)+'_eta'+str(eta)+'_eps'+str(eps)+'_maxS'+str(maxS)+'_ic'+ic+'_nr'+str(nr)+'_T'+str('%.3e' % T)+'_step'+str('%.e' % step)+'.npy'\n",
    "np.save(dirout+'S_av'+fstring,S_av)\n",
    "np.save(dirout+'m2_av'+fstring,m2_av)\n",
    "np.save(dirout+'m3_av'+fstring,m3_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-------- Agent-based model of social hierarchy: Kolmogorov-Smirnov (KS) test of compatibility of household income distributions with exponential distribution -------- #\n",
    "\n",
    "#Define function f: Root of f is maximum-likelihood parameter (T) for exponential distribution\n",
    "def f(T, xl, xh, sumx):\n",
    "    xl = p[0]\n",
    "    xh = p[1]\n",
    "    sumx = p[2]\n",
    "    return T + (xl-xh)*np.exp((xl-xh)/T)/(1-np.exp((xl-xh)/T)) - sumx\n",
    "\n",
    "#Define function to load-in and sort household income data and weights\n",
    "def select_data(xlow, xhigh):\n",
    "    #Load household income data and weights\n",
    "    dirname = '/home/jhickey/Shared/IncomeData/IPUMS/IPUMS_USA/numpy_files/'\n",
    "    y = np.load(dirname+'hhinc2015.npy')\n",
    "    w = np.load(dirname+'hhinc2015_weights.npy')\n",
    "    ypos = y[np.where(y>=0)]\n",
    "    wpos = w[np.where(y>=0)]\n",
    "    #Select data greater than xlow and smaller than xhigh\n",
    "    data_and_weights = np.array([np.copy(ypos[np.where((ypos>xlow) & (ypos<xhigh))]), \n",
    "                                 np.copy(wpos[np.where((ypos>xlow) & (ypos<xhigh))])])\n",
    "    #Sort by the \"data\" array, keeping the \"weights\" array tied to it        \n",
    "    sort_idx = np.argsort(data_and_weights[0])\n",
    "    data_and_weights = data_and_weights[:,sort_idx]\n",
    "    #Define separate, properly sorted, data and weights arrays\n",
    "    data = np.copy(data_and_weights)[0,:]\n",
    "    weights = np.copy(data_and_weights)[1,:]\n",
    "    return data, weights\n",
    "\n",
    "### Perform KS test ### \n",
    "\n",
    "#Define set of lower cutoffs (xlows) and upper cutoffs (xhighs) over which to perform the KS test\n",
    "xlow = 7e05\n",
    "xhigh = 1.1e06\n",
    "        \n",
    "#Initialize variables to catch case of extremely poor fit (where MLE parameter cannot be obtained)\n",
    "NaNpval = False\n",
    "NaNthresh = 1e07\n",
    "\n",
    "#Select data greater than xlow and smaller than xhigh\n",
    "data, weights = select_data(xlow, xhigh)\n",
    "\n",
    "#Define number of households in sample (N) and sum of weights (n)\n",
    "N = float(data.shape[0])\n",
    "n = float(np.sum(weights))\n",
    "\n",
    "#Define parameters for newton method root search\n",
    "sumx = 1/n*np.sum(weights*(data-xlow)) #include weights here        \n",
    "p = (xlow, xhigh, sumx)\n",
    "T0 = 1\n",
    "\n",
    "#Do a first test to see if Tdata is obtainable\n",
    "Tdata_check = scipy.optimize.newton(f, T0, args=p)\n",
    "if Tdata_check > NaNthresh or Tdata_check < -NaNthresh:\n",
    "    NaNpval=True    \n",
    "\n",
    "#Find Tdata using newton's method        \n",
    "Tdata = scipy.optimize.newton(f, T0, args=p)\n",
    "\n",
    "#Calculate Fdata (theoretical CCDF)\n",
    "Fdata = 1 - (1 - np.exp((xlow-data)/Tdata)) / (1 - np.exp((xlow-xhigh)/Tdata))        \n",
    "\n",
    "#Calculate Fn (empirical CCDF)\n",
    "Fn = []\n",
    "for t in data: Fn.append(len(data[data>t])/N)\n",
    "Fn = np.array(Fn)  \n",
    "\n",
    "#Calculate Fw (weighted empirical CCDF)\n",
    "Fw = []\n",
    "for t in data: \n",
    "    indices = np.where(data>t)[0]    \n",
    "    Fw.append(np.sum(weights[indices])/n)\n",
    "Fw = np.array(Fw)                \n",
    "\n",
    "#Find KS distance for the data\n",
    "Ddata = np.max(abs(Fdata-Fw))    \n",
    "\n",
    "## Find array of KS distances for synthetic datasets ##\n",
    "N_trials = 1000\n",
    "Dsynth = np.zeros(N_trials)        \n",
    "for trial in range(0,N_trials):  \n",
    "\n",
    "    #Generate synthetic data \"synth\" from exponential distribution with MLE parameter Tdata\n",
    "    u = np.random.random(int(N))    \n",
    "    synth = np.sort( xlow - Tdata * np.log(1 - (1-u)*(1-np.exp((xlow-xhigh)/Tdata))) ) \n",
    "\n",
    "    #Estimate Tsynth from the synth (this step needed to avoid bias, see doi:10.1142/S0219525909002131)\n",
    "    sumx_synth = 1/N*np.sum(synth-xlow)            \n",
    "    p = (xlow, xhigh, sumx_synth)\n",
    "    T0 = 1  \n",
    "    #Do a first test to see if Tsynth is obtainable\n",
    "    Tsynth_check = scipy.optimize.newton(f, T0, args=p)\n",
    "    if Tsynth_check > NaNthresh or Tsynth_check < -1*NaNthresh:\n",
    "        NaNpval = True\n",
    "        break             \n",
    "    Tsynth = scipy.optimize.newton(f, T0, args=p)\n",
    "    \n",
    "    #Calculate Fsynth (theoretical CCDF)\n",
    "    Fsynth = 1 - (1 - np.exp((xlow-synth)/Tsynth)) / (1 - np.exp((xlow-xhigh)/Tsynth))\n",
    "\n",
    "    #Calculate Fn_synth (empirical CCDF)\n",
    "    Fn_synth = 1. - np.arange(1,N+1)/N\n",
    "\n",
    "    #Find Dsynth (compared to Fw)\n",
    "    Dsynth[trial] = np.max(abs(Fsynth-Fn_synth))\n",
    "    \n",
    "if NaNpval == False: pval = len(np.where(Dsynth > Ddata)[0])/float(len(Dsynth))\n",
    "else: pval = float('NaN') \n",
    "        \n",
    "### Plot PDF of household income dist'n and fit of exp. dist'n with lower and upper cutoffs ###\n",
    "\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "ax1 = fig.add_subplot(121) #log-linear axis scale\n",
    "ax2 = fig.add_subplot(122) #log-log axis scale\n",
    "   \n",
    "#Plot PDF of household income data\n",
    "nbins=200\n",
    "hist, be = np.histogram(data, bins=np.logspace(0,np.log10(np.max(data)),nbins), density=True, weights=weights)\n",
    "xh = np.logspace(0,np.log10(np.max(data)),len(hist))\n",
    "ax1.plot(xh,np.log10(hist), '.-', label='data', linewidth=2)  \n",
    "ax2.plot(np.log10(xh),np.log10(hist), '.-', label='data', linewidth=2)\n",
    "\n",
    "#Plot PDF of theoretical fit (using parameter Tdata)\n",
    "yfit = 1./Tdata*np.exp((xlow-data)/Tdata)/(1-np.exp((xlow-xhigh)/Tdata))    \n",
    "ax1.plot(data,np.log10(yfit), '-r', label='fit, p-val = '+str(np.round(pval,2)), linewidth=2)\n",
    "ax2.plot(np.log10(data),np.log10(yfit), '-r', label='fit', linewidth=2)\n",
    "\n",
    "#Set the axis limits, axis labels, etc. on the figure\n",
    "ax1.set_xlim(xlow,xhigh)\n",
    "ax2.set_xlim(np.log10(xlow),np.log10(xhigh))\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_xlabel('$S$')\n",
    "ax1.set_ylabel('$\\log_{10}[p(S)]$')\n",
    "ax2.set_xlabel('$\\log_{10}[S]$')\n",
    "ax2.set_ylabel('$\\log_{10}[p(S)]$')\n",
    "ax1.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax2.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-------- Legal citation networks: Obtain temporal clustering of judgments by statistical inference of a generative model, using the expectation-maximization (EM) algorithm. (This is an application of the mixture model described in Section 2 of the paper by Leicht et al. at https://doi.org/10.1140/epjb/e2007-00271-7) -------- #\n",
    "\n",
    "#Load graph from saved\n",
    "lawtype = 'FAM'\n",
    "dirname = '/home/jhickey/Shared/Citn_ntwk/data/'+lawtype+'/saved_ntwks/'\n",
    "fname = lawtype+'_ntwk_20180901.xml.gz'\n",
    "g = gt.load_graph(dirname+fname)\n",
    "\n",
    "#Assign \"internal vertex properties\" to regular vertex properties \n",
    "citn = g.vp.citn_int\n",
    "date = g.vp.date_int\n",
    "crt = g.vp.crt_int\n",
    "ltype = g.vp.ltype_int\n",
    "title = g.vp.title_int\n",
    "tags = g.vp.tags_int\n",
    "crtrgnlvl = g.vp.crtrgnlvl_int\n",
    "\n",
    "#Assign internal edge properties to regular edge properties \n",
    "treatment = g.ep.treatment_int\n",
    "depth = g.ep.depth_int\n",
    "\n",
    "#Remove parallel edges, if any exist\n",
    "gt.remove_parallel_edges(g)\n",
    "\n",
    "#Filter to keep only vertices of ltype\n",
    "g.set_vertex_filter(ltype)\n",
    "\n",
    "#Filter to keep only vertices up to current_year\n",
    "current_year=2015\n",
    "g = gt.GraphView(g, vfilt=lambda v: date[v].year <= current_year)\n",
    "g = gt.GraphView(g, vfilt=gt.label_largest_component(g,directed=False))\n",
    "\n",
    "#Create dataframe to time-order the nodes in g\n",
    "datelist=[]\n",
    "vlist=[]\n",
    "degslist=[]\n",
    "for v in g.vertices(): \n",
    "    vlist.append(int(v))\n",
    "    datelist.append(date[v].year)\n",
    "    degslist.append(int(v.in_degree()))\n",
    "df = pd.DataFrame(data={'v': vlist, 'date': datelist, 'indegs': degslist})\n",
    "df=df.sort(columns='date').set_index(np.arange(df.shape[0]))\n",
    "\n",
    "#Define graph h, to be operated on\n",
    "h = gt.Graph(g)\n",
    "h.set_reversed(True)\n",
    "\n",
    "#Initiate variables\n",
    "start_year = df.date.iloc[0] #earliest year of a judgment in network h\n",
    "end_year = current_year+1 \n",
    "n = h.num_vertices() #number of nodes\n",
    "T = end_year-start_year #number of years\n",
    "z = np.zeros((n,T)) #number of citations from each judgment to each year\n",
    "\n",
    "#Determine z(i,t)\n",
    "for tup in df.v.iteritems():\n",
    "    i = df[df['v'] == tup[1]].index[0] #index indicating citing node \n",
    "    v = h.vertex(tup[1])\n",
    "    for e in v.in_edges():\n",
    "        u = e.source()\n",
    "        idx = df[df['v'] == int(u)].index[0] #index indicating cited node        \n",
    "        t = df.date.iloc[idx]-start_year\n",
    "        z[i,t] += 1\n",
    "k = np.sum(z, axis=1) #for use in calculation of theta_denom (below)\n",
    "\n",
    "#Initialize variables for EM procedure\n",
    "n_runs = 10 #average over this many runs (realizations)\n",
    "c_arr = np.arange(1,6) #c is the number of clusters \n",
    "\n",
    "for cidx, c in enumerate(c_arr):        \n",
    "        \n",
    "    q_prev = np.zeros((n,c)) #q array from previous iteration\n",
    "    theta_prev = np.zeros((c,T)) #theta array from previous iteration\n",
    "    ELs = np.zeros(n_runs)\n",
    "    H = np.zeros(n_runs)    \n",
    "    \n",
    "    for run in range(0,n_runs):\n",
    "               \n",
    "        #Initialize pi and theta\n",
    "        pi = 1/float(c)+np.random.random_sample(c)/10 #add random increment to uniform initial guess \n",
    "        pi = pi/sum(pi) #normalize\n",
    "        theta = 1/float(T)+np.random.random_sample((c,T))/10 #add random increment to uniform init. guess \n",
    "        theta = theta/np.sum(theta,axis=1) #normalize\n",
    "\n",
    "        #Initialize variables        \n",
    "        q = np.zeros((n,c))        \n",
    "        q_old = np.zeros((n,c))\n",
    "        beta = np.zeros((n,c))        \n",
    "        beta_max = np.zeros(n)  \n",
    "        \n",
    "        #Iterate q, theta, and pi until convergence in q\n",
    "        n_iter = 10000\n",
    "        start_time = time.time()\n",
    "        for m in range(0,n_iter):\n",
    "        \n",
    "            #Compute beta by matrix multiplication (define custom \"dot product\")            \n",
    "            beta = np.sum(np.log(theta[...,np.newaxis]**np.transpose(z)[np.newaxis,...]),axis=1) \n",
    "            beta = np.transpose(beta)        \n",
    "            \n",
    "            #Determine beta_max\n",
    "            beta_max = np.max(beta,axis=1)                    \n",
    "                \n",
    "            #Define cutoff (small value that will not cause underflow when exponentiated)\n",
    "            cutoff = np.log(1e-320)\n",
    "                        \n",
    "            #Compute numerator of q\n",
    "            q_numer = np.transpose(np.subtract(np.transpose(beta),beta_max))\n",
    "            q_numer[q_numer < cutoff] = float('-inf')\n",
    "            q_numer = pi*np.exp(q_numer)\n",
    "            #Compute denominator of q\n",
    "            q_denom = np.sum(q_numer,axis=1)\n",
    "            #Compute q\n",
    "            q = np.transpose(np.transpose(q_numer)/q_denom)\n",
    "                        \n",
    "            #Check q for convergence \n",
    "            if np.allclose(q,q_old,rtol=1e-5) == True: break          \n",
    "                \n",
    "            #Check for NaN in q \n",
    "            if True in np.isnan(q): \n",
    "                print \"NaN found 2\"\n",
    "                raise SystemExit\n",
    "                break\n",
    "                    \n",
    "            #Compute pi and theta\n",
    "            pi = 1/float(n)*np.sum(q,axis=0)\n",
    "            theta_numer=np.dot(np.transpose(q),z)\n",
    "            theta_denom = np.sum(np.transpose(np.tile(k,(c,1)))*q,axis=0)\n",
    "            theta = np.transpose(np.transpose(theta_numer)/theta_denom)\n",
    "                       \n",
    "            #Copy q to q_old, will use to check for convergence in next iteration\n",
    "            np.copyto(q_old,q)\n",
    "\n",
    "        #Once convergence reached, print following\n",
    "        print \"m =\", m, ' elapsed time =', time.time()-start_time                 \n",
    "                              \n",
    "        #Save each q output and theta_output            \n",
    "        fsave='_c'+str(c)+'_run'+str(run)+'_'+lawtype+'_y1'+str(start_year)+'_y2'+str(current_year)+'.npy'\n",
    "        np.save('/home/jhickey/outputdir/'+'q'+fsave,q)\n",
    "        np.save('/home/jhickey/outputdir/'+'theta'+fsave,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-------- Legal citation networks: Pre-process data and build graph with graph-tool package -------- #\n",
    "\n",
    "#Define function used to remove accented characters from strings\n",
    "def convert_string(string):\n",
    "    try:\n",
    "        string.decode('utf-8')\n",
    "        out = unidecode(string.decode('utf-8')) #input string is UTF-8\n",
    "    except UnicodeError:\n",
    "        try:\n",
    "            string.decode('latin-1')\n",
    "            out = unidecode(string.decode('latin-1')) #input string is Latin-1\n",
    "        except UnicodeError:\n",
    "            print \"string is not UTF-8 or Latin-1\"\n",
    "    return out\n",
    "\n",
    "#Initiate graph \"g\"\n",
    "g = gt.Graph()\n",
    "#Define vertex properties: \n",
    "citn=g.new_vertex_property(\"string\") #citation of judgment\n",
    "crt=g.new_vertex_property(\"string\") #court that issued judgment\n",
    "date=g.new_vertex_property(\"object\") #date of judgment (datetime format)\n",
    "title=g.new_vertex_property(\"string\") #title of judgment\n",
    "ltype=g.new_vertex_property(\"bool\") #used to filter judgments by area of law\n",
    "tags = g.new_vertex_property(\"object\") #list of topic tags attached to the judgment\n",
    "crtrgnlvl = g.new_vertex_property(\"string\") #e.g. \"ON-Tr\" for Ontario, Trial\n",
    "#Define edge properties:\n",
    "treatment = g.new_edge_property(\"string\") #treatment \n",
    "depth = g.new_edge_property(\"int\") #depth\n",
    "\n",
    "### Main Build-ntwk code ###\n",
    "#Each judgment in the \"cited_all.Citations\" series (loaded-in below) has a corresponding .csv file containing data on the judgments that cite it. This code builds the citation network from these .csv files as follows: for each \"cited\" judgment, open up its .csv file and assign links stemming from its \"citing\" judgments, creating new nodes each time a cited or citing judgment occurs that does not yet have a corresponding node in the network.\n",
    "\n",
    "#Choose dataset to use\n",
    "lawtype = 'DFM' \n",
    "dirname = '/home/jhickey/Shared/Citn_ntwk/data/'+lawtype+'/'\n",
    "\n",
    "#Read-in Title, Date, Citn for cited judgments (Carswell citation only)\n",
    "f = dirname+'lists_final/'+lawtype+'_cited_carswell.csv'\n",
    "cited_carswell = pd.read_csv(f, skiprows=1, names=['Title','Date','Citation_1','Citation_2'])\n",
    "#Read-in Title, Date, Citn for cited judgments (all citations: Carswell and non-Carswell) \n",
    "f = dirname+'lists_final/'+lawtype+'_cited_all.csv'\n",
    "cited_all = pd.read_csv(f,skiprows=1,names=['Title','Date','Citations'])\n",
    "\n",
    "#Add a semi-colon to end of every entry in \"cited_all.Citations\", to avoid error noted 2015-10-22\n",
    "cited_all.Citations.str.lower()+';'\n",
    "\n",
    "#Define replace_dict for cases where current_citing needs to be changed from Citation_2 to Citation_1\n",
    "replace_dict = {}\n",
    "for i in range(len(cited_carswell.Citation_2)):\n",
    "    if type(cited_carswell.Citation_2[i]) == str:\n",
    "        if cited_carswell.Citation_1[i].lower() not in replace_dict: \n",
    "            replace_dict[cited_carswell.Citation_2[i].lower()] = cited_carswell.Citation_1[i].lower()            \n",
    "\n",
    "#Open file containing citations for SDST_dict (\"SDST\" refers to judgments with the same date and same title, which need to be checked manually to determine if they are duplicates or not)\n",
    "f = dirname+'lists_final/'+lawtype+'_SDST_dictionary.csv'\n",
    "SDST_entries = pd.read_csv(f,skiprows=1,names=['Favoured','Unfavoured'])\n",
    "\n",
    "#Complete replace_dict by adding in SDST_entries not already present in replace_dict\n",
    "for i in range(0,len(SDST_entries.Favoured)):\n",
    "    if SDST_entries.Unfavoured[i] not in replace_dict:\n",
    "        replace_dict[SDST_entries.Unfavoured[i]] = SDST_entries.Favoured[i] \n",
    "\n",
    "#nan_citations list is used to store data for judgments (or other entries, e.g. \"CED\") appearing in .csv files with no citation (read-in as 'NaN')\n",
    "nan_citations = []\n",
    "\n",
    "#Count how many times crt-level gets filled in \n",
    "crt_fill_count = 0\n",
    "\n",
    "#Loop over all cited judgments (each one has its own .csv file of citing judgments)\n",
    "for idx_cited in range(0,len(cited_carswell.Citation_1)):\n",
    "    \n",
    "    #Move to current judgment, for which a .csv file exists\n",
    "    current_cited = cited_carswell.Citation_1[idx_cited].lower()\n",
    "    \n",
    "    #Correct current_cited if .csv file happens to contain \"Unfavoured\" replace_dict entry.\n",
    "    if current_cited in replace_dict: current_cited = replace_dict[current_cited]        \n",
    "    \n",
    "    #Check to see if a vertex already exists for this judgment, and if not add new vertex\n",
    "    x=gt.find_vertex(g,citn,current_cited) #x is a list of <class 'graph_tool.libgraph_tool_core.Vertex'>\n",
    "    if len(x) == 0:\n",
    "        #Add vertex\n",
    "        vcurrent_cited = g.add_vertex() #index of new vertex representing \"current_cited\" judgment\n",
    "        #Assign vertex properties\n",
    "        citn[vcurrent_cited] = current_cited\n",
    "        title[vcurrent_cited] = cited_carswell.Title[idx_cited]\n",
    "        #Put date in proper format\n",
    "        dt_cited = parse(cited_carswell.Date[idx_cited]).isoformat()[:10]\n",
    "        date[vcurrent_cited] = datetime.datetime.strptime(dt_cited, \"%Y-%m-%d\")\n",
    "         \n",
    "    #If vertex for this current_cited judgment already exists, then set vcurrent_cited appropriately\n",
    "    if len(x) == 1: vcurrent_cited = x[0]\n",
    "        \n",
    "    #Open file containing .csv spreadsheet and read-in data\n",
    "    fname_csv = dirname+lawtype+'_csv_all/'+current_cited+'.csv'\n",
    "    #Two lines below needed b/c of \"skipfooter\" error (instead of skipfooter in pd.read_csv, use nrows)\n",
    "    f_csv = open(fname_csv,'r')\n",
    "    num_lines = sum(1 for line in open(fname_csv))\n",
    "    \n",
    "    citing_refs=pd.read_csv(fname_csv, skiprows=1, nrows=num_lines-2, index_col=False, usecols=[0, 3, 5, 6, 7, 9, 11], names = ['Treatment', 'Title', 'Primary_Cite', 'Parallel_Cites', 'Court_or_Jurisdiction', 'Date', 'Depth'])\n",
    "\n",
    "    #Loop over all the entries in the current .csv file\n",
    "    for idx_citing in range(len(citing_refs.Primary_Cite)):\n",
    "        \n",
    "        current_citing = citing_refs.Primary_Cite[idx_citing]\n",
    "        \n",
    "        #Store judgments for which current_citing = 'NaN' and continue on to next idx_citing value\n",
    "        if type(current_citing) is not str:\n",
    "            if math.isnan(current_citing) == True: \n",
    "                nan_citations.append([current_cited, citing_refs.Title[idx_citing], citing_refs.Court_or_Jurisdiction[idx_citing], citing_refs.Parallel_Cites[idx_citing]])\n",
    "                continue\n",
    "            else: \n",
    "                print \"Error 1\"\n",
    "                #Stop program\n",
    "                raise SystemExit\n",
    "                    \n",
    "        #Replace current_citing with carswell version of citation, if possible\n",
    "        y = cited_all.Citations.str.contains(re.escape(current_citing))\n",
    "        if len(np.where(y==True)[0]) > 0:     \n",
    "            l = cited_all.Citations.iloc[y[y==True].index[0]].split(';')\n",
    "            current_citing = [val.strip().lower() for val in l if 'Carswell' in val][0]\n",
    "                \n",
    "        #Make sure current_citing is all lower-case \n",
    "        #(do after above \"NaN\"-related code b/c .lower() can't be applied to non-strings)\n",
    "        current_citing = current_citing.lower()                \n",
    "                \n",
    "        #Correct current_citing if .csv file contains Citation_2\n",
    "        if current_citing in replace_dict: current_citing = replace_dict[current_citing]            \n",
    "            \n",
    "        #Check if vertex for this current_citing judgment already exists, and if not add new vertex \n",
    "        x=gt.find_vertex(g,citn,current_citing)\n",
    "        if len(x) == 0:\n",
    "            #Add vertex \n",
    "            vcurrent_citing = g.add_vertex()\n",
    "            #Assign vertex properties\n",
    "            citn[vcurrent_citing] = current_citing\n",
    "            title[vcurrent_citing] = citing_refs.Title[idx_citing]\n",
    "            crt[vcurrent_citing] = citing_refs.Court_or_Jurisdiction[idx_citing]\n",
    "            #Handle dates with only year value (these are read-in as \"int\" instead of \"str\")\n",
    "            #Make month and day Jan. 01 by convention\n",
    "            if type(citing_refs.Date[idx_citing]) is not str and citing_refs.Date[idx_citing] > 1800 and citing_refs.Date[idx_citing] < 2000:\n",
    "                citing_refs.Date[idx_citing] = str(citing_refs.Date[idx_citing]) + \"-01-01\" \n",
    "            dt_citing = parse(citing_refs.Date[idx_citing]).isoformat()[:10] #put date in proper format\n",
    "            date[vcurrent_citing] = datetime.datetime.strptime(dt_cited, \"%Y-%m-%d\")\n",
    "        \n",
    "        #If vertex for this current_citing judgment already exists, set vcurrent_citing appropriately\n",
    "        if len(x) == 1: vcurrent_citing = x[0]\n",
    "        \n",
    "        #Assign \"crt\" vertex property if empty (b/c this inf. not directly avail. for cited jdgmnts)\n",
    "        if crt[vcurrent_citing] == \"\": \n",
    "            crt[vcurrent_citing] = citing_refs.Court_or_Jurisdiction[idx_citing]\n",
    "            crt_fill_count = crt_fill_count + 1\n",
    "    \n",
    "        #Add edge from vertex \"current_citing\" to vertex \"current_cited\"\n",
    "        e = g.add_edge(vcurrent_citing, vcurrent_cited)\n",
    "        \n",
    "        #Assign edge properties to the new edge\n",
    "        treatment[e] = citing_refs.Treatment[idx_citing]\n",
    "        depth[e] = citing_refs.Depth[idx_citing]\n",
    "        \n",
    "#Replace accented characters\n",
    "for v in g.vertices():\n",
    "    crt[v] = convert_string(crt[v])\n",
    "    title[v] = convert_string(title[v])\n",
    "    crtrgnlvl[v] = convert_string(crtrgnlvl[v])        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
